#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" && pwd)"
cd "$ROOT_DIR"

OVA_DIR="$ROOT_DIR/.ova"
BACKEND_PID="$OVA_DIR/backend.pid"
BACKEND_GROUP="$OVA_DIR/backend.group"
BACKEND_LOG="$OVA_DIR/backend.log"
FRONTEND_PID="$OVA_DIR/frontend.pid"
FRONTEND_GROUP="$OVA_DIR/frontend.group"
FRONTEND_LOG="$OVA_DIR/frontend.log"

BACKEND_PORT=5173
FRONTEND_PORT=8000

OVA_PROFILE="${OVA_PROFILE:-default}"
OVA_TTS_ENGINE="${OVA_TTS_ENGINE:-kokoro}"
OVA_LLM_BACKEND="${OVA_LLM_BACKEND:-ollama}"
OVA_LLM_MODEL="${OVA_LLM_MODEL:-}"
OVA_KOBOLDCPP_URL="${OVA_KOBOLDCPP_URL:-http://localhost:5001}"
OVA_POCKET_TTS_VOICE="${OVA_POCKET_TTS_VOICE:-alba}"

CHAT_MODEL="ministral-3:3b-instruct-2512-q4_K_M"

# HF models mapped by component
HF_MODEL_ASR="nvidia/parakeet-tdt-0.6b-v3"
HF_MODEL_KOKORO="hexgrad/Kokoro-82M"

usage() {
  cat <<'EOF'
Usage: ova [OPTIONS] <command>

Environment variables:
  OVA_PROFILE=<profile>              Set the profile to use (default: default)
  OVA_TTS_ENGINE=<engine>            TTS engine: kokoro, pocket_tts (default: kokoro)
  OVA_LLM_BACKEND=<backend>         LLM backend: ollama, koboldcpp (default: ollama)
  OVA_LLM_MODEL=<model>             LLM model name (default: auto per backend)
  OVA_KOBOLDCPP_URL=<url>           Koboldcpp API URL (default: http://localhost:5001)
  OVA_POCKET_TTS_VOICE=<voice>      Pocket-TTS voice: alba, marius, javert, jean, fantine, cosette, eponine, azelma (default: alba)

Commands:
  install        Install deps + download only the models needed for
                 your chosen OVA_TTS_ENGINE and OVA_LLM_BACKEND
  install --all  Install deps + download ALL models (every engine)
  start          Start backend + frontend server (non-blocking)
  stop           Stop running services
  status         Show whether services are running
  logs           Tail backend + frontend logs (Ctrl-C to quit)
  uninstall      Stop services and remove .venv + .ova
  reinstall      Uninstall then install again from scratch
  help           Show this message

Examples:
  ./ova.sh install                                        # only kokoro + ollama models (defaults)
  OVA_TTS_ENGINE=pocket_tts ./ova.sh install              # only pocket-tts + ollama models
  ./ova.sh install --all                                  # everything
  OVA_TTS_ENGINE=pocket_tts OVA_POCKET_TTS_VOICE=jean ./ova.sh start
  OVA_LLM_BACKEND=koboldcpp OVA_LLM_MODEL=my-model ./ova.sh start
EOF
}

die() {
  echo "ova: $*" >&2
  exit 1
}

ensure_cmd() {
  command -v "$1" >/dev/null 2>&1 || die "missing '$1' in PATH"
}

ensure_uv_lock() {
  [[ -f "$ROOT_DIR/uv.lock" ]] || die "uv.lock not found in project root"
}

ensure_env() {
  local env_file="$ROOT_DIR/.env"
  if [[ -f "$env_file" ]]; then
    echo ".env file already exists."
    return 0
  fi

  echo ""
  echo "No .env file found — let's create one."
  echo "A HuggingFace token is needed to download gated models."
  echo "Get yours at: https://huggingface.co/settings/tokens"
  echo ""
  printf "HF_TOKEN (paste token, or press Enter to skip): "
  read -r hf_token

  {
    echo "# Auto-generated by ova.sh install"
    echo ""
    echo "# HuggingFace token (required for downloading gated models)"
    echo "# Get yours at https://huggingface.co/settings/tokens"
    echo "HF_TOKEN=${hf_token}"
    echo ""
    echo "# OVA configuration (these can also be set as shell env vars)"
    echo "# OVA_PROFILE=default"
    echo "# OVA_TTS_ENGINE=kokoro            # kokoro | pocket_tts"
    echo "# OVA_LLM_BACKEND=ollama           # ollama | koboldcpp"
    echo "# OVA_LLM_MODEL=                   # model name (auto-detected per backend if empty)"
    echo "# OVA_KOBOLDCPP_URL=http://localhost:5001"
    echo "# OVA_POCKET_TTS_VOICE=alba        # alba | marius | javert | jean | fantine | cosette | eponine | azelma"
  } > "$env_file"

  echo ".env file created."
  if [[ -n "$hf_token" ]]; then
    echo "HF_TOKEN has been saved."
  else
    echo "HF_TOKEN left blank — you can edit .env later to add it."
  fi
  echo ""
}

ensure_pip() {
  if uv run python3 - <<'PY'
try:
    import pip  # noqa: F401
except Exception as exc:
    raise SystemExit(1) from exc
PY
  then
    echo "pip already available in uv venv"
    return 0
  fi

  echo "Installing pip into uv venv"
  if uv run python3 -m ensurepip --upgrade; then
    return 0
  fi
  uv pip install --upgrade pip
}

is_running() {
  local pidfile=$1
  [[ -f "$pidfile" ]] || return 1
  local pid
  pid="$(cat "$pidfile" 2>/dev/null || true)"
  [[ "$pid" =~ ^[0-9]+$ ]] || return 1
  ps -p "$pid" >/dev/null 2>&1
}

port_open() {
  local port=$1
  python3 - <<PY
import socket
import sys

port = int("${port}")
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
sock.settimeout(0.2)
try:
    sock.connect(("127.0.0.1", port))
    sock.close()
    sys.exit(0)
except Exception:
    sys.exit(1)
PY
}

wait_for_port() {
  local name=$1
  local port=$2
  local pidfile=$3
  local logfile=$4
  local timeout=${5:-10}
  local success_msg=${6:-}

  local start
  start="$(date +%s)"
  while true; do
    if ! is_running "$pidfile"; then
      echo "$name failed to start (process exited). See log: $logfile"
      return 1
    fi

    if port_open "$port"; then
      if [[ -n "$success_msg" ]]; then
        echo "$success_msg"
      else
        echo "$name started successfully (port $port)"
      fi
      return 0
    fi

    local now
    now="$(date +%s)"
    if (( now - start >= timeout )); then
      echo "$name failed to start within ${timeout}s. See log: $logfile"
      return 1
    fi

    sleep 0.25
  done
}

wait_for_log_message() {
  local name=$1
  local logfile=$2
  local pidfile=$3
  local message=$4
  local attempts=${5:-120}
  local interval=${6:-1}
  local success_msg=${7:-}

  for ((i=1; i<=attempts; i++)); do
    if ! is_running "$pidfile"; then
      echo "$name failed to start (process exited). See log: $logfile"
      return 1
    fi

    if [[ -f "$logfile" ]] && grep -Fq "$message" "$logfile"; then
      if [[ -n "$success_msg" ]]; then
        echo "$success_msg"
      else
        echo "$name started successfully"
      fi
      return 0
    fi

    sleep "$interval"
  done

  echo "$name failed to start within ${attempts}s. See log: $logfile"
  return 1
}

start_detached() {
  local logfile=$1
  local pidfile=$2
  local groupfile=$3
  shift 3

  mkdir -p "$OVA_DIR"
  : > "$logfile"

  if command -v setsid >/dev/null 2>&1; then
    setsid "$@" >"$logfile" 2>&1 < /dev/null &
    echo "$!" > "$pidfile"
    echo "1" > "$groupfile"
  else
    nohup "$@" >"$logfile" 2>&1 < /dev/null &
    echo "$!" > "$pidfile"
    rm -f "$groupfile"
  fi
}

stop_service() {
  local name=$1
  local pidfile=$2
  local groupfile=$3

  if [[ ! -f "$pidfile" ]]; then
    echo "$name not running"
    return 0
  fi

  local pid
  pid="$(cat "$pidfile" 2>/dev/null || true)"
  if [[ -z "$pid" || ! "$pid" =~ ^[0-9]+$ ]]; then
    rm -f "$pidfile" "$groupfile"
    echo "$name not running"
    return 0
  fi

  if ! ps -p "$pid" >/dev/null 2>&1; then
    rm -f "$pidfile" "$groupfile"
    echo "$name not running"
    return 0
  fi

  if [[ -f "$groupfile" ]]; then
    kill -- -"$pid" >/dev/null 2>&1 || true
  else
    kill "$pid" >/dev/null 2>&1 || true
  fi

  for _ in {1..25}; do
    if ps -p "$pid" >/dev/null 2>&1; then
      sleep 0.2
    else
      break
    fi
  done

  if ps -p "$pid" >/dev/null 2>&1; then
    if [[ -f "$groupfile" ]]; then
      kill -9 -- -"$pid" >/dev/null 2>&1 || true
    else
      kill -9 "$pid" >/dev/null 2>&1 || true
    fi
  fi

  rm -f "$pidfile" "$groupfile"
  echo "$name stopped"
}

ensure_ollama_model() {
  local model=$1
  local models
  if models="$(ollama list 2>/dev/null || true)"; then
    if echo "$models" | awk 'NR>1{print $1}' | grep -qx "$model"; then
      echo "Ollama model present: $model"
      return 0
    fi
  fi

  echo "Pulling Ollama model: $model"
  ollama pull "$model"
}

ensure_hf_model() {
  local repo_id=$1
  ensure_cmd uvx
  local cache_list
  cache_list="$(uvx hf cache list 2>/dev/null || true)"
  if [[ -n "$cache_list" ]] && echo "$cache_list" | grep -Fq "$repo_id"; then
    echo "HF model present: $repo_id"
  else
    echo "Downloading HF model: $repo_id"
    uvx hf download "$repo_id"
  fi
}

ensure_pocket_tts() {
  echo "Pre-warming Pocket-TTS model cache..."
  uv run python3 -c "
from pocket_tts import TTSModel
model = TTSModel.load_model()
state = model.get_state_for_audio_prompt('alba')
print('Pocket-TTS model cached successfully.')
" 2>/dev/null && echo "Pocket-TTS ready." || echo "Pocket-TTS pre-warm skipped (will download on first use)."
}

install_models() {
  local install_all=${1:-false}

  # ASR model is always needed
  ensure_hf_model "$HF_MODEL_ASR"

  # TTS models — only download what's needed (or everything with --all)
  if [[ "$install_all" == "true" || "$OVA_TTS_ENGINE" == "kokoro" ]]; then
    ensure_hf_model "$HF_MODEL_KOKORO"
  fi
  if [[ "$install_all" == "true" || "$OVA_TTS_ENGINE" == "pocket_tts" ]]; then
    ensure_pocket_tts
  fi

  # LLM models — only pull Ollama model when using Ollama (or --all)
  if [[ "$install_all" == "true" || "$OVA_LLM_BACKEND" == "ollama" ]]; then
    if command -v ollama >/dev/null 2>&1; then
      ensure_ollama_model "$CHAT_MODEL"
    elif [[ "$install_all" == "true" ]]; then
      echo "WARNING: ollama not found in PATH — skipping Ollama model download."
      echo "  Install Ollama (https://ollama.com) and run:  ollama pull $CHAT_MODEL"
    else
      die "missing 'ollama' in PATH (needed for OVA_LLM_BACKEND=ollama)"
    fi
  fi
}

cmd="${1:-help}"
subcmd="${2:-}"

case "$cmd" in
  install)
    ensure_cmd uv
    ensure_uv_lock
    ensure_env

    echo "Installing Python dependencies..."
    uv sync --frozen
    ensure_pip

    install_all="false"
    if [[ "$subcmd" == "--all" ]]; then
      install_all="true"
      echo "Installing ALL models (every engine)..."
    else
      echo "Installing models for: TTS=$OVA_TTS_ENGINE, LLM=$OVA_LLM_BACKEND"
      echo "(use './ova.sh install --all' to download everything)"
    fi

    install_models "$install_all"
    echo "Install complete."
    ;;
  start)
    ensure_cmd uv
    ensure_uv_lock
    echo "Starting Outrageous Voice Assistant..."
    echo "  Profile:        $OVA_PROFILE"
    echo "  TTS Engine:     $OVA_TTS_ENGINE"
    echo "  LLM Backend:    $OVA_LLM_BACKEND"
    [[ -n "$OVA_LLM_MODEL" ]] && echo "  LLM Model:      $OVA_LLM_MODEL"
    [[ "$OVA_TTS_ENGINE" == "pocket_tts" ]] && echo "  Pocket-TTS Voice: $OVA_POCKET_TTS_VOICE"
    [[ "$OVA_LLM_BACKEND" == "koboldcpp" ]] && echo "  Koboldcpp URL:  $OVA_KOBOLDCPP_URL"

    echo "Starting front-end..."
    if is_running "$FRONTEND_PID"; then
      if port_open "$FRONTEND_PORT"; then
        echo "Front-end already running (port $FRONTEND_PORT)"
      else
        echo "Front-end running but not responding on port $FRONTEND_PORT"
      fi
    else
      start_detached "$FRONTEND_LOG" "$FRONTEND_PID" "$FRONTEND_GROUP" \
        uv run python3 -m http.server --directory "$ROOT_DIR"
      wait_for_port "Web server" "$FRONTEND_PORT" "$FRONTEND_PID" "$FRONTEND_LOG" 5 \
        "Front-end started successfully (port $FRONTEND_PORT)"
    fi

    echo "Starting back-end..."
    if is_running "$BACKEND_PID"; then
      if port_open "$BACKEND_PORT"; then
        echo "Back-end already running (port $BACKEND_PORT)"
      else
        echo "Back-end running but not responding on port $BACKEND_PORT"
      fi
    else
      start_detached "$BACKEND_LOG" "$BACKEND_PID" "$BACKEND_GROUP" \
        bash -c "OVA_PROFILE='$OVA_PROFILE' OVA_TTS_ENGINE='$OVA_TTS_ENGINE' OVA_LLM_BACKEND='$OVA_LLM_BACKEND' OVA_LLM_MODEL='$OVA_LLM_MODEL' OVA_KOBOLDCPP_URL='$OVA_KOBOLDCPP_URL' OVA_POCKET_TTS_VOICE='$OVA_POCKET_TTS_VOICE' uv run uvicorn ova.api:app --reload --port \"$BACKEND_PORT\""
      wait_for_log_message "Backend" "$BACKEND_LOG" "$BACKEND_PID" \
        "Application startup complete." 60 1 \
        "Back-end started successfully (port $BACKEND_PORT)"
    fi
    echo "All services are up and running."
    echo "Now just fire up your browser and go to http://localhost:$FRONTEND_PORT and enjoy!!!"
    ;;
  stop)
    stop_service "Backend" "$BACKEND_PID" "$BACKEND_GROUP"
    stop_service "Web server" "$FRONTEND_PID" "$FRONTEND_GROUP"
    ;;
  status)
    echo "OVA Status"
    echo "=========="
    if is_running "$BACKEND_PID"; then
      if port_open "$BACKEND_PORT"; then
        echo "Backend:    running (port $BACKEND_PORT)"
      else
        echo "Backend:    running (port $BACKEND_PORT NOT responding)"
      fi
    else
      echo "Backend:    stopped"
    fi
    if is_running "$FRONTEND_PID"; then
      if port_open "$FRONTEND_PORT"; then
        echo "Frontend:   running (port $FRONTEND_PORT)"
      else
        echo "Frontend:   running (port $FRONTEND_PORT NOT responding)"
      fi
    else
      echo "Frontend:   stopped"
    fi
    echo ""
    echo "Config"
    echo "------"
    echo "  Profile:          $OVA_PROFILE"
    echo "  TTS Engine:       $OVA_TTS_ENGINE"
    echo "  LLM Backend:      $OVA_LLM_BACKEND"
    [[ -n "$OVA_LLM_MODEL" ]] && echo "  LLM Model:        $OVA_LLM_MODEL"
    [[ "$OVA_LLM_BACKEND" == "koboldcpp" ]] && echo "  Koboldcpp URL:    $OVA_KOBOLDCPP_URL"
    echo ""
    echo "Paths"
    echo "-----"
    echo "  Project:   $ROOT_DIR"
    echo "  Logs:      $OVA_DIR/"
    [[ -d "$ROOT_DIR/.venv" ]] && echo "  Venv:      $ROOT_DIR/.venv" || echo "  Venv:      (not installed)"
    ;;
  logs)
    if [[ ! -d "$OVA_DIR" ]]; then
      die "no logs found — has the service been started? (run ./ova.sh start)"
    fi
    echo "Tailing backend + frontend logs (Ctrl-C to quit)..."
    echo ""
    tail -f "$BACKEND_LOG" "$FRONTEND_LOG" 2>/dev/null || \
      die "log files not found in $OVA_DIR"
    ;;
  uninstall)
    echo "Stopping services..."
    stop_service "Backend" "$BACKEND_PID" "$BACKEND_GROUP"
    stop_service "Web server" "$FRONTEND_PID" "$FRONTEND_GROUP"

    if [[ -d "$ROOT_DIR/.venv" ]]; then
      echo "Removing virtual environment..."
      rm -rf "$ROOT_DIR/.venv"
      echo "Virtual environment removed."
    else
      echo "No virtual environment found."
    fi

    if [[ -d "$OVA_DIR" ]]; then
      echo "Removing runtime data (.ova)..."
      rm -rf "$OVA_DIR"
      echo "Runtime data removed."
    else
      echo "No runtime data found."
    fi

    echo ""
    echo "Uninstall complete."
    echo "Note: HuggingFace model cache and Ollama models are stored"
    echo "globally and were not removed. To free disk space, run:"
    echo "  uvx hf cache delete"
    echo "  ollama rm $CHAT_MODEL"
    ;;
  reinstall)
    echo "Reinstalling OVA..."
    "$0" uninstall
    echo ""
    "$0" install "$subcmd"
    ;;
  help|-h|--help)
    usage
    ;;
  *)
    die "unknown command: $cmd"
    ;;
esac
